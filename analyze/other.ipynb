{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dcd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_dict_schema(d1: dict, d2: dict) -> bool:\n",
    "    if d1.keys() != d2.keys():\n",
    "        return False\n",
    "    for k in d1:\n",
    "        v1, v2 = d1[k], d2[k]\n",
    "        if isinstance(v1, dict) and isinstance(v2, dict):\n",
    "            if not same_dict_schema(v1, v2):\n",
    "                return False\n",
    "        else:\n",
    "            if type(v1) is not type(v2):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# 示例\n",
    "a = {\"id\": 1, \"info\": {\"name\": \"Tom\", \"age\": 20}}\n",
    "b = {\"id\": 999, \"info\": {\"name\": \"Alice\", \"age\": 5}}\n",
    "c = {\"info\": {\"name\": \"Tom\", \"age\": 1}, \"id\": 1}  # age 类型不同\n",
    "\n",
    "print(same_dict_schema(a, b))  # True\n",
    "print(same_dict_schema(a, c))  # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5897d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "COMMON_STOP_TOKENS = {\"请\", \"帮\", \"帮我\", \"一下\", \"的\", \"请问\"}\n",
    "\n",
    "\n",
    "def _clean(text: str) -> str:\n",
    "    # 去掉全角空格与常见标点\n",
    "    return re.sub(r\"[，。,！？?；;：:\\s]\", \"\", text)\n",
    "\n",
    "\n",
    "def _lcs_indices(a: str, b: str) -> set:\n",
    "    \"\"\"返回 a 与 b 的一个 LCS 中 a 的字符索引集合（不唯一，取任一即可）\"\"\"\n",
    "    la, lb = len(a), len(b)\n",
    "    dp = [[0]*(lb+1) for _ in range(la+1)]\n",
    "    for i in range(la):\n",
    "        for j in range(lb):\n",
    "            if a[i] == b[j]:\n",
    "                dp[i+1][j+1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i+1][j+1] = max(dp[i][j+1], dp[i+1][j])\n",
    "    # 回溯\n",
    "    i, j = la, lb\n",
    "    indices = set()\n",
    "    while i > 0 and j > 0:\n",
    "        if a[i-1] == b[j-1]:\n",
    "            indices.add(i-1)\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        else:\n",
    "            if dp[i-1][j] >= dp[i][j-1]:\n",
    "                i -= 1\n",
    "            else:\n",
    "                j -= 1\n",
    "    return indices\n",
    "\n",
    "\n",
    "def extract_unique_phrase(main_query: str, alt_queries: List[str]) -> str:\n",
    "    main = _clean(main_query)\n",
    "    if not alt_queries:\n",
    "        return main\n",
    "    alt_cleaned = [_clean(q) for q in alt_queries]\n",
    "\n",
    "    # 计算每个备选的 LCS 覆盖索引，再取交集（模板位置）\n",
    "    lcs_sets = [_lcs_indices(main, alt) for alt in alt_cleaned]\n",
    "    template_positions = set.intersection(*lcs_sets) if lcs_sets else set()\n",
    "\n",
    "    # 非模板位置就是候选\n",
    "    variable_positions = [i for i in range(\n",
    "        len(main)) if i not in template_positions]\n",
    "\n",
    "    if not variable_positions:\n",
    "        return \"\"\n",
    "\n",
    "    # 合并连续片段\n",
    "    segments: List[Tuple[int, int]] = []\n",
    "    start = variable_positions[0]\n",
    "    prev = start\n",
    "    for pos in variable_positions[1:]:\n",
    "        if pos == prev + 1:\n",
    "            prev = pos\n",
    "        else:\n",
    "            segments.append((start, prev + 1))\n",
    "            start = pos\n",
    "            prev = pos\n",
    "    segments.append((start, prev + 1))\n",
    "\n",
    "    # 取中文字符片段并做停用词/噪声过滤\n",
    "    def valid_seg(s: str) -> bool:\n",
    "        if not s:\n",
    "            return False\n",
    "        if all(ch in COMMON_STOP_TOKENS for ch in s):\n",
    "            return False\n",
    "        # 至少含一个汉字\n",
    "        return bool(re.search(r\"[\\u4e00-\\u9fff]\", s))\n",
    "\n",
    "    candidates = []\n",
    "    for a, b in segments:\n",
    "        seg = main[a:b]\n",
    "        # 去掉首尾常见功能字\n",
    "        seg = re.sub(r\"^(请|帮我|帮|一下|的)+\", \"\", seg)\n",
    "        seg = re.sub(r\"(请|帮我|帮|一下|的)+$\", \"\", seg)\n",
    "        if valid_seg(seg):\n",
    "            candidates.append(seg)\n",
    "\n",
    "    if not candidates:\n",
    "        return \"\"\n",
    "\n",
    "    # 返回最长（或可再加权）\n",
    "    candidates.sort(key=lambda x: len(x), reverse=True)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_q = \"包含朱一龙燕之屋关键词的笔记词云表现，近30天\"\n",
    "    alts = [\"包含路觅教育关键词的笔记词云表现，近30天\",\n",
    "            \"包含法式床关键词的笔记词云表现，近30天\", \"查询时间为近90天，关键词为 茅台 的热搜词\"]\n",
    "    print(extract_unique_phrase(main_q, alts))  # 期望输出: 雅诗兰黛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693699c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import jieba\n",
    "import re\n",
    "from typing import List, Tuple,Dict,Set\n",
    "from collections import Counter\n",
    "from math import log\n",
    "# ================== 模板停用词（基础集合） ==================\n",
    "TEMPLATE_STOP = {\n",
    "    # 基础功能\n",
    "    \"请\", \"帮\", \"帮我\", \"一下\", \"的\", \"请问\", \"需要\", \"我想查询\", \"我想要\", \"我想要抓取\", \"我想看\", \"拉取\", \"查询\", \"查看\", \"统计\", \"导出\", \"输出\", \"提供\", \"求和\", \"分析\", \"加以分析\",\n",
    "    # 查询结构 & 条件\n",
    "    \"包含\", \"模糊匹配\", \"筛选\", \"筛选条件\", \"条件\", \"关键词\", \"关键词为\", \"关键词包含\", \"搜索词\", \"搜索词内容\", \"竞价词\", \"品牌\", \"名称\", \"名称和\", \"名称及\", \"各\", \"以及\", \"对比\", \"分组\", \"分组展示\", \"维度\", \"分搜索词维度\", \"指标\",\n",
    "    # 时间/范围提示词\n",
    "    \"时间\", \"时间为\", \"时间是\", \"查询时间\", \"时间范围\", \"时间周期\", \"期间\", \"至今\", \"到现在\", \"全年\", \"月底\", \"年初\", \"月份\", \"月份的\", \"月份内\", \"范围\",\n",
    "    # 时间单位（碎片防拼接）\n",
    "    \"年\", \"月\", \"日\", \"号\", \"天\", \"季度\",\n",
    "    # 相对时间\n",
    "    \"昨日\", \"昨天\", \"今日\", \"今天\", \"本月\", \"上个月\", \"今年\", \"去年\", \"近\", \"近半年\", \"近一个月\", \"近两个月\", \"近三个月\", \"近六个月\", \"近12个月\", \"近1个月\", \"近一年来\", \"近一年\",\n",
    "    # 趋势 & 图表\n",
    "    \"趋势\", \"趋势图\", \"指数\", \"指数趋势\", \"指数趋势图\", \"搜索趋势\", \"搜索趋势图\", \"折线\", \"折线图\", \"折线趋势\", \"折线趋势图\", \"榜单\", \"排名\", \"TOP\", \"TOP3\",\n",
    "    # 搜索指标\n",
    "    \"搜索\", \"搜索量\", \"搜索次数\", \"搜索人数\", \"搜索指数\", \"搜索指数趋势\", \"搜索指数趋势图\",\n",
    "    # 热搜 / 上下游\n",
    "    \"热搜词\", \"热搜词有哪些\", \"热搜词都有哪些\", \"相关\", \"相关的\", \"相关热搜词\",\n",
    "    \"上下游词\", \"上下游搜索词\", \"上下游词数据表现\", \"上下游搜索词数据表现\", \"数据表现\", \"数据\",\n",
    "    # SOV / SOC\n",
    "    \"sov\", \"soc\", \"SOV\", \"SOC\", \"值\", \"排名\",\n",
    "    # 内容/笔记\n",
    "    \"笔记\", \"热门笔记\", \"笔记数\", \"有效笔记\", \"有效\", \"新增\", \"数量\", \"数量趋势\", \"标题\", \"正文\", \"标签\",\n",
    "    \"阅读\", \"阅读量\", \"曝光\", \"曝光数\", \"曝光量\", \"点击\", \"点击数\", \"点击量\", \"互动\", \"互动量\", \"点赞\", \"收藏\", \"评论\",\n",
    "    \"累计曝光\", \"累计阅读\", \"累计点赞\", \"累计收藏\", \"累计评论\",\n",
    "    \"日均\", \"按日均\", \"每日\", \"每天\", \"按月\", \"按月份\", \"每个月\", \"每一个月\", \"分月\", \"按月统计\", \"按月份统计\", \"by月\", \"byday\", \"按天\",\n",
    "    # 账号 / 作者 / 店铺 / 品牌\n",
    "    \"账号\", \"账号名称\", \"账号ID\", \"作者\", \"作者ID\", \"店铺\", \"店铺gmv\", \"gmv\", \"GMV\", \"品牌搜索\", \"品牌搜索趋势\",\n",
    "    # AIPS\n",
    "    \"aips\", \"资产\", \"规模\", \"消耗\", \"新增aips\", \"人群\", \"人群资产\", \"人群资产规模\",\n",
    "    # 其他结构\n",
    "    \"输出\", \"导出\", \"统计\", \"查询\", \"需要\", \"提供\", \"隐去\", \"只保留\", \"只给\", \"加以\", \"分析\",\n",
    "    # 去噪单字（必要时可扩展）\n",
    "    \"图\", \"月\", \"日\", \"天\",\n",
    "    # 词云\n",
    "    \"词云表现\", \"词云\"\n",
    "}\n",
    "\n",
    "# 可选：若想彻底避免 “搜索量” 被拆成 “搜索” + “量” 后留下 “量”，可开启下面单字停用\n",
    "SINGLE_STOP_EXTRA = {\"次\"}  # 按需加入/删除\n",
    "TEMPLATE_STOP |= SINGLE_STOP_EXTRA\n",
    "\n",
    "# 需要强制保持整体的多字模板短语（越长越靠前）\n",
    "MANDATORY_LONG_TEMPLATE_PHRASES = {\n",
    "    \"搜索指数趋势图\", \"搜索指数趋势\", \"搜索指数\", \"搜索量\", \"搜索次数\", \"搜索人数\",\n",
    "    \"上下游搜索词数据表现\", \"上下游词数据表现\", \"上下游搜索词\",\n",
    "    \"品牌搜索趋势\", \"品牌搜索\", \"数量趋势\", \"指数趋势图\", \"指数趋势\",\n",
    "    \"脱敏社区搜索量\", \"有效笔记\", \"热门笔记\"\n",
    "}\n",
    "\n",
    "# 初始化：把多字模板词按长度降序加到 jieba 词典，避免拆分\n",
    "\n",
    "\n",
    "def init_template_phrases():\n",
    "    long_phrases = sorted(\n",
    "        {w for w in (TEMPLATE_STOP | MANDATORY_LONG_TEMPLATE_PHRASES)\n",
    "         if len(w) > 1},\n",
    "        key=lambda x: len(x),\n",
    "        reverse=True\n",
    "    )\n",
    "    for w in long_phrases:\n",
    "        # 设置极高频率强制不再拆\n",
    "        jieba.add_word(w, freq=10**8, tag=\"tpl\")\n",
    "    # 避免单字被过度误杀的情况，可视需要移除 SINGLE_STOP_EXTRA 中某些字符\n",
    "    # print(\"Loaded template phrases:\", long_phrases[:10], \"... total:\", len(long_phrases))\n",
    "\n",
    "\n",
    "# 在模块加载时初始化\n",
    "init_template_phrases()\n",
    "\n",
    "# ================= 时间正则（修正前两条拼接错误） =================\n",
    "TIME_SPAN_PATTERNS = [\n",
    "    # 1. 年月日 到 年月日（后端可缺年份）\n",
    "    r'(?:20\\d{2}|19\\d{2}|[0-9]{2})年\\d{1,2}月\\d{1,2}[日号]?\\s*[到至\\-—~～]\\s*(?:(?:20\\d{2}|19\\d{2}|[0-9]{2})年)?(?:\\d{1,2}月)?\\d{1,2}[日号]?',\n",
    "    # 2. 年月 到 年月（后段可缺年份）\n",
    "    r'(?:20\\d{2}|19\\d{2}|[0-9]{2})年\\d{1,2}月\\s*[到至\\-—~～]\\s*(?:(?:20\\d{2}|19\\d{2}|[0-9]{2})年)?\\d{1,2}月',\n",
    "    # 3. 年月日到现在/至今\n",
    "    r'(?:20\\d{2}|19\\d{2})年\\d{1,2}月\\d{1,2}[日号]?到(?:现在|至今)',\n",
    "    r'(?:20\\d{2}|19\\d{2})年\\d{1,2}月到(?:现在|至今)',\n",
    "    r'今年\\d{1,2}月份?到(?:现在|至今)',\n",
    "    r'从(?:20\\d{2}|19\\d{2})年\\d{1,2}月\\d{1,2}[日号]?到(?:现在|至今)',\n",
    "    r'从今年\\d{1,2}月份?到(?:现在|至今)',\n",
    "    # 4. 简写年份跨月日\n",
    "    r'(?:[0-9]{2})年\\d{1,2}月\\d{1,2}[日号]?[\\-—~～到至]\\d{1,2}月\\d{1,2}[日号]?',\n",
    "    # 5. 年-月范围\n",
    "    r'(?:20\\d{2}|19\\d{2}|[0-9]{2})年\\d{1,2}月[\\-—~～到至]\\d{1,2}月',\n",
    "    r'(?:20\\d{2}|19\\d{2}|[0-9]{2})年\\d{1,2}[\\-—~～到至]\\d{1,2}月',\n",
    "    # 6. 年x月份\n",
    "    r'(?:20\\d{2}|19\\d{2}|[0-9]{2})年\\d{1,2}月份?',\n",
    "    # 7. 年月日\n",
    "    r'(?:20\\d{2}|19\\d{2}|[0-9]{2})年\\d{1,2}月\\d{1,2}[日号]',\n",
    "    # 8. 年月\n",
    "    r'(?:20\\d{2}|19\\d{2}|[0-9]{2})年\\d{1,2}月',\n",
    "    # 9. 纯数字连续范围\n",
    "    r'20\\d{6}[到至\\-—~～]20\\d{6}',\n",
    "    r'20\\d{6}[到至\\-—~～]\\d{4}',\n",
    "    r'20\\d{6}至\\d{6}',\n",
    "    # 10. 点号日期范围\n",
    "    r'(?:20\\d{2}|19\\d{2})[\\.年]\\d{1,2}\\.\\d{1,2}[日号]?[\\-—~～到至]\\d{1,2}\\.\\d{1,2}[日号]?',\n",
    "    r'(?:20\\d{2}|19\\d{2})\\.\\d{1,2}\\.?[\\-—~～到至](?:20\\d{2}|19\\d{2})?\\.\\d{1,2}',\n",
    "    # 11. 年+季度\n",
    "    r'(?:20\\d{2}|19\\d{2})年Q[1-4][到至\\-—~～](?:20\\d{2}|19\\d{2})?年?Q[1-4]',\n",
    "    # 12. 年范围 / 单年\n",
    "    r'(?:20\\d{2}|19\\d{2})年[到至\\-—~～](?:20\\d{2}|19\\d{2})年',\n",
    "    r'(?:20\\d{2}|19\\d{2})年',\n",
    "    # 13. 月日-月日\n",
    "    r'\\d{1,2}月\\d{1,2}[日号]?[\\-—~～到至]\\d{1,2}月\\d{1,2}[日号]?',\n",
    "    r'\\(\\d{1,2}月\\d{1,2}[日号]?[\\-—~～到至]\\d{1,2}月\\d{1,2}[日号]?\\)',\n",
    "    r'半年\\(\\d{1,2}月\\d{1,2}[日号]?[\\-—~～到至]\\d{1,2}月\\d{1,2}[日号]?\\)',\n",
    "    # 15. 单日（无年）\n",
    "    r'\\d{1,2}月\\d{1,2}[日号]',\n",
    "    # 16. 月-月\n",
    "    r'\\d{1,2}月[\\-—~～到至]\\d{1,2}月',\n",
    "    r'\\d{1,2}[\\-—~～到至]\\d{1,2}月',\n",
    "    # 17. 月底 / 年初\n",
    "    r'(?:20\\d{2}|19\\d{2})年\\d{1,2}月底',\n",
    "    r'(?:20\\d{2}|19\\d{2})年年初',\n",
    "    # 18. 相对时间（宽）\n",
    "    r'(?:近|最近|过去)\\s*[一二三四五六七八九十百千万\\d]+\\s*(?:天|日|周|个月|月|季度|年|年来)',\n",
    "    r'近\\s*[一二三四五六七八九十]+\\s*年(?:来)?',\n",
    "    # 19. 相对天数\n",
    "    r'近\\s*\\d+\\s*日',\n",
    "    r'近\\s*\\d+\\s*天',\n",
    "    r'最近\\s*\\d+\\s*天',\n",
    "    r'最近\\s*\\d+\\s*日',\n",
    "    # 20. 单词相对\n",
    "    r'(?:今日|昨天|昨日|上个月|本月|今年|去年|近半年|近一个月|近两个月|近三个月|近六个月|近12个月)',\n",
    "    # 21. 至今\n",
    "    r'至今',\n",
    "    r'到现在',\n",
    "    # 22. 全年\n",
    "    r'(?:20\\d{2}|19\\d{2})年全年',\n",
    "    # 23. 月号到月号\n",
    "    r'\\d{1,2}月\\d{1,2}号[到至\\-—~～]\\d{1,2}月\\d{1,2}[号日]',\n",
    "    # 24. 年+起止（冗余补充）\n",
    "    r'(?:20\\d{2}|19\\d{2})年\\d{1,2}月\\d{1,2}[日号]?[\\-—~～到至]\\d{1,2}月\\d{1,2}[日号]?',\n",
    "]\n",
    "\n",
    "TIME_SPAN_REGEXES = [re.compile(p) for p in TIME_SPAN_PATTERNS]\n",
    "\n",
    "RELATIVE_TIME_TOKEN = re.compile(\n",
    "    r'^(?:近|最近|过去)[一二三四五六七八九十百千万\\d]+(?:天|日|周|个月|月|季度|年|年来)?$'\n",
    ")\n",
    "\n",
    "def _find_time_spans(text: str) -> List[Tuple[int,int]]:\n",
    "    matches = []\n",
    "    for rx in TIME_SPAN_REGEXES:\n",
    "        for m in rx.finditer(text):\n",
    "            matches.append((m.start(), m.end()))\n",
    "    if not matches:\n",
    "        return []\n",
    "    # 合并重叠，优先长\n",
    "    matches.sort(key=lambda x:(x[0], - (x[1]-x[0])))\n",
    "    merged = []\n",
    "    cur_s, cur_e = matches[0]\n",
    "    for s,e in matches[1:]:\n",
    "        if s <= cur_e:  # overlap\n",
    "            if e > cur_e:\n",
    "                cur_e = e\n",
    "        else:\n",
    "            merged.append((cur_s, cur_e))\n",
    "            cur_s, cur_e = s,e\n",
    "    merged.append((cur_s, cur_e))\n",
    "    return merged\n",
    "\n",
    "def _mask_time_spans(text: str) -> str:\n",
    "    spans = _find_time_spans(text)\n",
    "    if not spans:\n",
    "        return text\n",
    "    pieces = []\n",
    "    last = 0\n",
    "    for idx,(s,e) in enumerate(spans):\n",
    "        if s > last:\n",
    "            pieces.append(text[last:s])\n",
    "        pieces.append(f'T_TIME{idx}')  # 占位\n",
    "        last = e\n",
    "    if last < len(text):\n",
    "        pieces.append(text[last:])\n",
    "    return ''.join(pieces)\n",
    "\n",
    "def _is_time_token(tok: str) -> bool:\n",
    "    if tok.startswith('T_TIME'):\n",
    "        return True\n",
    "    if RELATIVE_TIME_TOKEN.fullmatch(tok):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "SEP_PATTERN = re.compile(r\"[、，,、/\\s]+\")\n",
    "\n",
    "DIGIT_PATTERN = re.compile(r\"^\\d+$\")\n",
    "\n",
    "SLOT_PATTERNS = [\n",
    "    re.compile(r\"包含(?P<slot>.+?)关键词\"),\n",
    "    re.compile(r\"关键词为\\s*(?P<slot>[\\u4e00-\\u9fffA-Za-z0-9 ]+)\"),\n",
    "    re.compile(r\"关键词包含(?P<slot>.+?)的\"),\n",
    "]\n",
    "\n",
    "def _normalize_text(q: str) -> str:\n",
    "    return re.sub(r\"[，。,！？?；;：:\\s]\", \"\", q)\n",
    "\n",
    "\n",
    "def _segment(q: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    1. 先时间占位 mask\n",
    "    2. 去除常规标点（新增去掉 '、'）\n",
    "    3. jieba 分词\n",
    "    4. 拆分被粘连的占位符：例如 socT_TIME0 -> soc + T_TIME0\n",
    "    5. 去除纯标点/空白\n",
    "    \"\"\"\n",
    "    masked = _mask_time_spans(q)\n",
    "    cleaned = re.sub(r\"[，。,！？?；;：:\\s、]\", \"\", masked)\n",
    "    toks = [w for w in jieba.lcut(cleaned) if w.strip()]\n",
    "    new = []\n",
    "    for t in toks:\n",
    "        if \"T_TIME\" in t and not t.startswith(\"T_TIME\"):\n",
    "            # 拆分占位符\n",
    "            parts = re.split(r'(T_TIME\\d+)', t)\n",
    "            for p in parts:\n",
    "                if not p:\n",
    "                    continue\n",
    "                new.append(p)\n",
    "        else:\n",
    "            new.append(t)\n",
    "    # 再次把仍含 T_TIME 且前后粘连的情况切干净（保险）\n",
    "    final = []\n",
    "    for t in new:\n",
    "        if \"T_TIME\" in t and t != re.findall(r'T_TIME\\d+', t)[0]:\n",
    "            # 若还有粘连（极少），强制剥离所有占位符\n",
    "            buf = re.split(r'(T_TIME\\d+)', t)\n",
    "            for b in buf:\n",
    "                if b:\n",
    "                    final.append(b)\n",
    "        else:\n",
    "            final.append(t)\n",
    "    # 去掉孤立的 '、'\n",
    "    return [w for w in final if w != \"、\"]\n",
    "\n",
    "def _extract_slot_candidates(q: str) -> List[str]:\n",
    "    candidates = []\n",
    "    for pat in SLOT_PATTERNS:\n",
    "        for m in pat.finditer(q):\n",
    "            raw = m.group(\"slot\").strip()\n",
    "            if not raw:\n",
    "                continue\n",
    "            raw = raw.strip(\"“”\\\"' \")\n",
    "            parts = [p for p in SEP_PATTERN.split(raw) if p]\n",
    "            if parts:\n",
    "                candidates.append(\"\".join(parts))\n",
    "    return candidates\n",
    "\n",
    "def extract_unique_phrase(main_query: str,\n",
    "                          alt_queries: List[str],\n",
    "                          jaccard_threshold: float = 0.15,\n",
    "                          majority_threshold: float = 0.6,\n",
    "                          return_all: bool = True) -> List[str]:\n",
    "    \"\"\"\n",
    "    时间片段预掩码 + 多数模板判定 + 槽位融合\n",
    "    \"\"\"\n",
    "    if not alt_queries:\n",
    "        return [main_query]\n",
    "\n",
    "    slot_main = _extract_slot_candidates(main_query)\n",
    "\n",
    "    main_tokens = _segment(main_query)\n",
    "    alt_tokens_list = [_segment(q) for q in alt_queries]\n",
    "\n",
    "    main_set = set(main_tokens)\n",
    "\n",
    "    # 过滤相似度过低的备选\n",
    "    valid_alts = []\n",
    "    for toks in alt_tokens_list:\n",
    "        s = set(toks)\n",
    "        jac = len(main_set & s) / (len(main_set | s) or 1)\n",
    "        if jac >= jaccard_threshold:\n",
    "            valid_alts.append(toks)\n",
    "\n",
    "    if not valid_alts:\n",
    "        if slot_main:\n",
    "            return slot_main\n",
    "        filtered = [t for t in main_tokens if t not in TEMPLATE_STOP and not _is_time_token(t)]\n",
    "        return [\"\".join(filtered)] if filtered else [\"\"]\n",
    "\n",
    "    # 统计 DF\n",
    "    df: Dict[str,int] = {}\n",
    "    for toks in valid_alts:\n",
    "        for w in set(toks):\n",
    "            df[w] = df.get(w,0) + 1\n",
    "    alt_count = len(valid_alts)\n",
    "\n",
    "    # 判定模板 token\n",
    "    template_tokens: Set[str] = set()\n",
    "    for w in main_tokens:\n",
    "        freq = df.get(w,0)/alt_count\n",
    "        if (freq >= majority_threshold) or (w in TEMPLATE_STOP) or _is_time_token(w):\n",
    "            template_tokens.add(w)\n",
    "\n",
    "    # 标记变量\n",
    "    variable_flags = [w not in template_tokens for w in main_tokens]\n",
    "\n",
    "    # 合并连续变量\n",
    "    segments = []\n",
    "    cur=[]\n",
    "    for flag,tok in zip(variable_flags, main_tokens):\n",
    "        if flag:\n",
    "            cur.append(tok)\n",
    "        else:\n",
    "            if cur:\n",
    "                segments.append(cur)\n",
    "                cur=[]\n",
    "    if cur:\n",
    "        segments.append(cur)\n",
    "\n",
    "    candidates=[]\n",
    "    for seg in segments:\n",
    "        clean = [\n",
    "            w for w in seg if w not in TEMPLATE_STOP and not _is_time_token(w)]\n",
    "        clean = [w for w in clean if not DIGIT_PATTERN.match(w)]\n",
    "        if not clean:\n",
    "            continue\n",
    "        phrase = \"\".join(clean)\n",
    "        # 去掉首尾时间残余边界符\n",
    "        phrase = re.sub(r'^[日至号到至\\-—~～、]+', '', phrase)\n",
    "        phrase = re.sub(r'[日至号到至\\-—~～、]+$', '', phrase)\n",
    "        # 去掉嵌入的占位符或其截断残片\n",
    "        phrase = re.sub(r'T_TIME\\d+', '', phrase)\n",
    "        # 去掉剥离后可能残留的单独 'T'\n",
    "        phrase = re.sub(r'^T+$', '', phrase)\n",
    "        # 再次清理可能产生的前后符号\n",
    "        phrase = re.sub(r'^[日至号到至\\-—~～、]+', '', phrase)\n",
    "        phrase = re.sub(r'[日至号到至\\-—~～、]+$', '', phrase)\n",
    "        if not phrase:\n",
    "            continue\n",
    "        if len(phrase) == 1 and phrase in TEMPLATE_STOP:\n",
    "            continue\n",
    "        candidates.append(phrase)\n",
    "\n",
    "    # 融合槽位\n",
    "    for s in slot_main:\n",
    "        if s and s not in candidates:\n",
    "            candidates.append(s)\n",
    "\n",
    "    # 去伪变量：在多数 alt 中完整出现的剔除\n",
    "    filtered=[]\n",
    "    for c in candidates:\n",
    "        appear = sum(1 for q in alt_queries if c in q)\n",
    "        if appear >= alt_count*0.6:\n",
    "            continue\n",
    "        filtered.append(c)\n",
    "\n",
    "    def score(c: str):\n",
    "        in_slot = 1 if c in slot_main else 0\n",
    "        appear = sum(1 for q in alt_queries if c in q)\n",
    "        uniqueness = -appear\n",
    "        return (in_slot, len(c), uniqueness)\n",
    "\n",
    "    final=[]\n",
    "    seen=set()\n",
    "    for c in sorted(filtered, key=score, reverse=True):\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            final.append(c)\n",
    "\n",
    "    return final if return_all else (final[:1] or [\"\"])\n",
    "\n",
    "\n",
    "# 测试\n",
    "if __name__ == \"__main__\":\n",
    "    tests = [\n",
    "        (\"包含朱一龙燕之屋关键词的笔记词云表现，近30天\", [\n",
    "            \"包含路觅教育关键词的笔记词云表现，近30天\",\n",
    "            \"包含法式床关键词的笔记词云表现，近30天\",\n",
    "            \"查询时间为近90天，关键词为 茅台 的热搜词\",\n",
    "            \"儿童液体钙类目下，包含液体钙、ad关键词的热门笔记，近30天\",\n",
    "            \"近一年来，用户搜索关键词模糊匹配“泡泡玛特股票”的搜索量，需要每一个月的\"\n",
    "        ]),\n",
    "        (\"包含VIPKID关键词的笔记词云表现，近30天\", [\n",
    "            \"包含路觅教育关键词的笔记词云表现，近30天\",\n",
    "            \"包含法式床关键词的笔记词云表现，近30天\"\n",
    "        ]),\n",
    "        (\"包含HOLLISTER关键词的笔记词云表现，近8个月\", [\n",
    "            \"包含路觅教育关键词的笔记词云表现，近30天\",\n",
    "            \"查询时间为近90天，关键词包含 茅台1935 的热搜词\"\n",
    "        ]),\n",
    "    ]\n",
    "    for mq, alts in tests:\n",
    "        print(mq, \"->\", extract_unique_phrase(mq, alts))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_q = \"包含朱一龙燕之屋关键词的笔记词云表现，近30天\"\n",
    "    alts = [\n",
    "        \"包含路觅教育关键词的笔记词云表现，近30天\",\n",
    "        \"包含法式床关键词的笔记词云表现，近30天\",\n",
    "        \"查询时间为近90天，关键词为 朱一龙燕之屋 的热搜词\"\n",
    "    ]\n",
    "    print(extract_unique_phrase(main_q, alts))  # 期望: ['朱一龙燕之屋']\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e83587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "target_file_url = '../log/20250919/144659_only_retrieved.json'\n",
    "with open(target_file_url, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "len(data)\n",
    "target_key = [key for key in data.keys() if 'entity' in key][0]\n",
    "data = data[target_key]\n",
    "retrieved = data[\"retrieved\"]\n",
    "ct = {}\n",
    "for retrieved_item in retrieved:\n",
    "    pattern = retrieved_item[\"pattern\"]\n",
    "    query = pattern[\"query\"]\n",
    "    pattern_retrieved = pattern[\"retrieved\"]\n",
    "    retrieved_query = [pr[0]['query'] for pr in pattern_retrieved]\n",
    "    ct[query] = retrieved_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8696a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "for main_q, alts in list(ct.items()):\n",
    "    print(\"main_q:\", main_q, \"alts:\", alts)\n",
    "    print(\"extracted:\", extract_unique_phrase(main_q, alts))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5482cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "template_file_url = '../data/20250916/template_AIO_0.95_4.json'\n",
    "test_file_url = '../data/20250916/test_AIO_0.95_4.json'\n",
    "with open(template_file_url, 'r', encoding='utf-8') as f:\n",
    "    templates = json.load(f)\n",
    "with open(test_file_url, 'r', encoding='utf-8') as f:\n",
    "    tests = json.load(f)\n",
    "len(templates), len(tests)\n",
    "template_query = [t[\"query\"] for t in templates]\n",
    "test_query = [t[\"query\"] for t in tests]\n",
    "querys = template_query + test_query\n",
    "\n",
    "querys_file_url = '../data/20250916/template_test_querys.txt'\n",
    "with open(querys_file_url, 'w', encoding='utf-8') as f:\n",
    "    f.writelines(\"\\n\".join(querys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154dbb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"../data/ignore/20250916_AIO_correct_erased_embedding.parquet\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1c0c4",
   "metadata": {},
   "source": [
    "## 处理spider相关的错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fb074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "target_file = \"../data/spider_dsl/test/spider_sim_value.json\"\n",
    "with open(target_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "len(data)\n",
    "# 569 503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    assert item.get(\"id\"), f\"Missing id in item {item['question']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a065727",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for item in data:\n",
    "    id = item['config'].get(\"id\") or item.get(\"id\")\n",
    "    item[\"id\"] = int(id)+569+569\n",
    "    print(id, item)\n",
    "    item[\"table_name\"] = item['config'].get(\"table_name\") or item.get(\"table_name\")\n",
    "    item[\"config\"] = {\n",
    "        \"dimension\": item['config'].get(\"dimension\", []),\n",
    "        \"measure\": item['config'].get(\"measure\", []),\n",
    "        \"filter\": item['config'].get(\"filter\", [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed682f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/spider_dsl/test/spider_sim_value_clean.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"../data/spider_dsl/test/spider_sim_value_clean.json\",\n",
    "         \"../data/spider_dsl/test/spider_sim_column_clean.json\",\n",
    "         \"../data/spider_dsl/test/spider_sim_question_clean.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e10e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = sum([json.load(open(file, \"r\", encoding='utf-8')) for file in files], [])\n",
    "\n",
    "with open(\"../data/spider_dsl/test/spider_sim_merged.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff2744e",
   "metadata": {},
   "source": [
    "## 处理bird相关的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f55c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "target_file = \"../data/bird_dsl/test/bird_sim_question.json\"\n",
    "with open(target_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "len(data)\n",
    "# 569 503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    assert item.get(\"id\"), f\"Missing id in item {item['question']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for item in data:\n",
    "    id = item['config'].get(\"id\") or item.get(\"id\")\n",
    "    item[\"id\"] = int(id)+353*3\n",
    "    # print(id, item)\n",
    "    item[\"table_name\"] = item['config'].get(\n",
    "        \"table_name\") or item.get(\"table_name\")\n",
    "    item[\"config\"] = {\n",
    "        \"dimension\": item['config'].get(\"dimension\", []),\n",
    "        \"measure\": item['config'].get(\"measure\", []),\n",
    "        \"filter\": item['config'].get(\"filter\", [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9760d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(target_file.rpartition('.')[0] + '_clean.json', \"w\", encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
